# Task 11: API - Chat Endpoint

**Status**: [ ] TODO
**Estimated Time**: 4 hours
**Dependencies**: Tasks 01-10 (Complete pipeline + Vector DB)
**Priority**: HIGH
**File**: `api/chat.js`

---

## ðŸ“‹ Description

Create the main chat API endpoint that implements the RAG (Retrieval-Augmented Generation) pipeline to answer user questions based on the knowledge base.

---

## ðŸŽ¯ Goals

1. Create `/api/chat` POST endpoint
2. Implement RAG pipeline (query â†’ retrieve â†’ generate)
3. Integrate with vector database for retrieval
4. Call OpenRouter LLM for response generation
5. Return formatted response with sources
6. Handle errors gracefully
7. Add request validation

---

## âœ… Acceptance Criteria

- [ ] POST `/api/chat` accepts JSON requests
- [ ] Validates input (message required, max length)
- [ ] Generates query embedding
- [ ] Retrieves top-K relevant chunks from vector DB
- [ ] Constructs context for LLM
- [ ] Calls OpenRouter API with proper prompts
- [ ] Returns JSON response with answer + sources
- [ ] Response time < 3 seconds for 95% of requests
- [ ] Handles errors with appropriate status codes

---

## ðŸ”§ Implementation

### api/chat.js

```javascript
const { vectorDB } = require('../lib/vectordb')
const { generateEmbedding } = require('../lib/embeddings')
const axios = require('axios')

/**
 * Main chat endpoint
 */
module.exports = async (req, res) => {
  // Only accept POST
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  try {
    const { message, conversationId, language = 'ja' } = req.body

    // Validate input
    if (!message || typeof message !== 'string') {
      return res.status(400).json({ error: 'Message is required' })
    }

    if (message.length > 1000) {
      return res.status(400).json({ error: 'Message too long (max 1000 chars)' })
    }

    console.log(`[Chat] Received message: ${message.substring(0, 50)}...`)

    // STEP 1: Generate query embedding
    const queryEmbedding = await generateEmbedding(message)

    // STEP 2: Vector search
    const searchResults = await vectorDB.query({
      vector: queryEmbedding,
      topK: 5,
      includeMetadata: true
    })

    if (!searchResults || searchResults.length === 0) {
      return res.json({
        response: 'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚',
        sources: [],
        conversationId: conversationId || generateId()
      })
    }

    console.log(`[Chat] Found ${searchResults.length} relevant chunks`)

    // STEP 3: Build context from retrieved chunks
    const context = searchResults
      .map((result, idx) => {
        return `[Source ${idx + 1}] ${result.metadata.content}\n(Timestamp: ${result.metadata.timestamp})`
      })
      .join('\n\n')

    // STEP 4: Generate response with LLM
    const systemPrompt = `ã‚ãªãŸã¯é’æœ¨ã•ã‚“ã®æ•™ãˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã‹ã¤ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

é‡è¦ãªãƒ«ãƒ¼ãƒ«:
1. æä¾›ã•ã‚ŒãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„
3. å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€è©²å½“ã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã—ã¦ãã ã•ã„
4. ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªžã§ç­”ãˆã¦ãã ã•ã„
5. é’æœ¨ã•ã‚“ã®è¨€è‘‰ã‚„æ•™ãˆã‚’å°Šé‡ã—ã¦ãã ã•ã„

çŸ¥è­˜ãƒ™ãƒ¼ã‚¹:
${context}`

    const llmResponse = await callOpenRouter({
      system: systemPrompt,
      user: message,
      model: 'openai/gpt-4o-mini-2024-07-18'
    })

    // STEP 5: Format sources
    const sources = searchResults.map(result => ({
      text: result.metadata.content.substring(0, 200) + '...',
      timestamp: result.metadata.timestamp,
      topic: result.metadata.topic,
      relevanceScore: result.score
    }))

    // Return response
    return res.status(200).json({
      response: llmResponse,
      sources,
      conversationId: conversationId || generateId(),
      metadata: {
        retrievedChunks: searchResults.length,
        processingTime: Date.now() - startTime
      }
    })

  } catch (error) {
    console.error('[Chat] Error:', error)

    return res.status(500).json({
      error: 'Internal server error',
      message: error.message
    })
  }
}

/**
 * Call OpenRouter API
 */
async function callOpenRouter({ system, user, model }) {
  const response = await axios.post(
    'https://openrouter.ai/api/v1/chat/completions',
    {
      model,
      messages: [
        { role: 'system', content: system },
        { role: 'user', content: user }
      ],
      temperature: 0.7,
      max_tokens: 500
    },
    {
      headers: {
        'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://your-app.vercel.app',
        'X-Title': 'Voice Chatbot'
      }
    }
  )

  return response.data.choices[0].message.content
}

/**
 * Generate conversation ID
 */
function generateId() {
  return `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
}
```

---

## ðŸ§ª Testing Checklist

### Unit Tests

```javascript
// Mock test for API endpoint logic
describe('Chat API', () => {
  test('validates required message field', async () => {
    const req = { method: 'POST', body: {} }
    const res = { status: jest.fn(() => ({ json: jest.fn() })) }

    await chatHandler(req, res)

    expect(res.status).toHaveBeenCalledWith(400)
  })

  test('rejects non-POST requests', async () => {
    const req = { method: 'GET', body: {} }
    const res = { status: jest.fn(() => ({ json: jest.fn() })) }

    await chatHandler(req, res)

    expect(res.status).toHaveBeenCalledWith(405)
  })
})
```

### Integration Tests

- [ ] **Test with real question**:
  ```bash
  curl -X POST http://localhost:3000/api/chat \
    -H "Content-Type: application/json" \
    -d '{
      "message": "é’æœ¨ã•ã‚“ãŒ29æ­³ã®æ™‚ã«å‡ºä¼šã£ãŸã‚‚ã®ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    }'
  ```

- [ ] **Expected response**:
  ```json
  {
    "response": "é’æœ¨ã•ã‚“ã¯29æ­³ã§ãƒã‚¤ãƒ–ãƒ«ï¼ˆè–æ›¸ï¼‰ã¨å‡ºä¼šã„ã¾ã—ãŸã€‚ãã“ã§é»„é‡‘çŽ‡ã¨ã„ã†æ•™ãˆã‚’çŸ¥ã‚Šã€äººç”ŸãŒå¤‰ã‚ã‚Šã¾ã—ãŸã€‚",
    "sources": [
      {
        "text": "29æ­³ã§ãƒã‚¤ãƒ–ãƒ«ã¨å‡ºä¼šã£ã¦...",
        "timestamp": "00:01:19,320 --> 00:01:44,880",
        "topic": "é»„é‡‘çŽ‡ã¨ä¾¡å€¤è¦³",
        "relevanceScore": 0.92
      }
    ],
    "conversationId": "conv_1234567890_abc123"
  }
  ```

### Manual Testing

Test with these questions:

1. **Factual**: "é’æœ¨ã•ã‚“ãŒ29æ­³ã®æ™‚ã«å‡ºä¼šã£ãŸã‚‚ã®ã¯ä½•ã§ã™ã‹ï¼Ÿ"
   - Should return: ãƒã‚¤ãƒ–ãƒ«, é»„é‡‘çŽ‡
   - Should cite correct timestamp

2. **Conceptual**: "é»„é‡‘çŽ‡ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
   - Should return full explanation with quote
   - Should reference ãƒžã‚¿ã‚¤7ç« 12ç¯€

3. **Advice**: "ä¾¡å€¤è¦³ãŒåˆã‚ãªã„äººã¨ã©ã†ä»˜ãåˆã†ã¹ãã§ã™ã‹ï¼Ÿ"
   - Should return practical advice
   - Should include relevant quotes

4. **Unknown**: "é’æœ¨ã•ã‚“ã®å¥½ããªé£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿ"
   - Should return "æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“"

5. **Edge cases**:
   - Empty message â†’ 400 error
   - Very long message (>1000 chars) â†’ 400 error
   - Invalid JSON â†’ 400 error

---

## ðŸ“Š Performance Requirements

- [ ] **Response time**: < 3 seconds for 95% of requests
  - Vector search: < 500ms
  - LLM generation: < 2s
  - Total: < 3s

- [ ] **Accuracy**: > 85% relevant responses
  - Test with 20 questions
  - Verify sources are correct

- [ ] **Error handling**: All errors return proper status codes
  - 400: Bad request
  - 405: Method not allowed
  - 500: Internal error

---

## âš™ï¸ Environment Variables Required

```env
OPENROUTER_API_KEY=your_key_here
PINECONE_API_KEY=your_key_here
PINECONE_INDEX_NAME=transcript-knowledge
OPENAI_API_KEY=your_key_here
```

---

## âš ï¸ Common Issues & Solutions

### Issue: Vector search returns no results
- **Cause**: Index not populated or different namespace
- **Solution**: Verify DB is populated, check namespace

### Issue: LLM response is hallucinated
- **Cause**: Weak system prompt
- **Solution**: Strengthen prompt to stick to knowledge base only

### Issue: Timeout (>10s on Vercel)
- **Cause**: Slow LLM or vector search
- **Solution**: Reduce topK, optimize prompts, use faster model

---

## âœ¨ Success Criteria

Task is complete when:
1. âœ… API endpoint created and deployed
2. âœ… Can answer test questions accurately
3. âœ… Returns proper sources with timestamps
4. âœ… Response time < 3 seconds
5. âœ… Error handling works correctly
6. âœ… All integration tests pass

---

## ðŸ“Œ Next Task

**Task 12: API - Health & Initialize Endpoints** (`tasks/backend/12-api-other.md`)

---

**Status**: [ ] TODO
**Started**: _____
**Completed**: _____
